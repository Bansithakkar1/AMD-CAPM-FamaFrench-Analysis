{"metadata":{"kernelspec":{"display_name":"Python [conda env:base] *","language":"python","name":"conda-base-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.9"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"19b47333-0f1b-48db-84e7-d2a4e405c8c8","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy import stats\n\n# --- Load stock returns ---\namd = pd.read_excel('AMD - Stock return.xlsx').dropna(subset=['Stock Return'])\namd['Date'] = pd.to_datetime(amd['Date'])\n\n# --- Load Fama-French factors ---\nff = pd.read_excel('F-F_Research_Data_Factors.xlxs.xlsx', skiprows=3)\nff.columns = ['Date', 'Mkt-RF', 'SMB', 'HML', 'RF']\nff = ff.dropna(subset=['Date'])\nff['Date'] = pd.to_numeric(ff['Date'], errors='coerce')\nff = ff.dropna(subset=['Date'])\nff['Date'] = ff['Date'].astype(int).astype(str)\nff = ff[ff['Date'].str.len() == 6]\nff['Date'] = pd.to_datetime(ff['Date'], format='%Y%m')\nff[['Mkt-RF', 'SMB', 'HML', 'RF']] /= 100\n\n# --- Merge and compute excess return ---\ndata = pd.merge(amd, ff, on='Date', how='inner')\ndata['AMD_Excess'] = data['Stock Return'] - data['RF']\n\nfor col in ['AMD_Excess', 'Mkt-RF', 'SMB', 'HML']:\n    data[col] = pd.to_numeric(data[col], errors='coerce')\ndata = data.dropna(subset=['AMD_Excess', 'Mkt-RF', 'SMB', 'HML'])","metadata":{"trusted":true},"outputs":[],"execution_count":32},{"id":"a8c7ca3c-3949-454b-a4c4-c4728042f934","cell_type":"code","source":"# QUESTION 1: CAPM REGRESSION\ny = data['AMD_Excess']\nX = sm.add_constant(data[['Mkt-RF']])\ncapm = sm.OLS(y, X).fit()\n\nbeta = capm.params['Mkt-RF']\nalpha = capm.params['const']\nr2 = capm.rsquared\nalpha_pval = capm.pvalues['const']\n\nprint(capm.summary())\n\n# Task 1: Beta\nprint(f\"\\nBeta = {beta:.4f}\")\n\n# Task 2: Alpha significance\nprint(f\"Alpha = {alpha:.4f} (p-value = {alpha_pval:.4f})\")\nprint(\"Significant at 5%\" if alpha_pval < 0.05 else \"Not significant at 5%\")\n\n# Task 3: R-squared\nprint(f\"R² = {r2:.4f} ({r2*100:.1f}% of variance explained by market)\")\n\n# Task 4: Expected excess return if market = 10%\nexpected = alpha + beta * 0.10\nprint(f\"Expected excess return = {expected:.4f}\")","metadata":{"trusted":true,"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":"                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             AMD_Excess   R-squared:                       0.347\nModel:                            OLS   Adj. R-squared:                  0.332\nMethod:                 Least Squares   F-statistic:                     22.86\nDate:                Wed, 18 Feb 2026   Prob (F-statistic):           2.07e-05\nTime:                        00:40:53   Log-Likelihood:                 30.184\nNo. Observations:                  45   AIC:                            -56.37\nDf Residuals:                      43   BIC:                            -52.75\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0047      0.019      0.247      0.806      -0.034       0.043\nMkt-RF         1.8923      0.396      4.781      0.000       1.094       2.691\n==============================================================================\nOmnibus:                        4.701   Durbin-Watson:                   2.316\nProb(Omnibus):                  0.095   Jarque-Bera (JB):                3.783\nSkew:                           0.699   Prob(JB):                        0.151\nKurtosis:                       3.254   Cond. No.                         21.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nBeta = 1.8923\nAlpha = 0.0047 (p-value = 0.8061)\nNot significant at 5%\nR² = 0.3471 (34.7% of variance explained by market)\nExpected excess return = 0.1939\n"}],"execution_count":33},{"id":"aa7252d4-0383-4b40-8537-95cab01afa93","cell_type":"code","source":"# QUESTION 2: COMPARING TWO STOCKS (AMD vs Berkshire Hathaway)\n\n# Load Berkshire data\nberk = pd.read_excel(\"Berkshire Hathaway Inc.xlsx\")\nberk['Date'] = pd.to_datetime(berk['Date'])\nberk['Stock_Return'] = berk.iloc[:, 5].pct_change()\nberk = berk.dropna(subset=['Stock_Return'])\n\n# Normalize dates to first of month for matching\nberk['Date'] = berk['Date'].to_numpy().astype('datetime64[M]')\n\n# Merge with FF\nm_berk = pd.merge(berk[['Date', 'Stock_Return']], ff[['Date', 'Mkt-RF', 'RF']],\n                   on='Date', how='inner')\nm_berk['Excess'] = m_berk['Stock_Return'] - m_berk['RF']\n\n# Force numeric\nfor col in ['Excess', 'Mkt-RF', 'RF']:\n    m_berk[col] = pd.to_numeric(m_berk[col], errors='coerce')\nm_berk = m_berk.dropna(subset=['Excess', 'Mkt-RF'])\n\n# Task 1: CAPM for Berkshire\nX_berk = sm.add_constant(m_berk['Mkt-RF'])\ncapm_berk = sm.OLS(m_berk['Excess'], X_berk).fit()\n\nprint(\"AMD:\")\nprint(capm.summary())\nprint(\"\\nBERKSHIRE:\")\nprint(capm_berk.summary())\n\nprint(f\"\\nAMD Beta: {capm.params['Mkt-RF']:.4f}\")\nprint(f\"Berkshire Beta: {capm_berk.params['Mkt-RF']:.4f}\")\n\n# Task 2: F-test for equal betas\nY_pool = np.concatenate([data['AMD_Excess'].values, m_berk['Excess'].values])\nX_pool = np.concatenate([data['Mkt-RF'].values, m_berk['Mkt-RF'].values])\nD = np.concatenate([np.ones(len(data)), np.zeros(len(m_berk))])\nN = len(Y_pool)\n\nX_r = sm.add_constant(np.column_stack([X_pool, D]))\nX_ur = sm.add_constant(np.column_stack([X_pool, D, D * X_pool]))\n\nm_r = sm.OLS(Y_pool, X_r).fit()\nm_ur = sm.OLS(Y_pool, X_ur).fit()\n\nf_stat = ((m_r.ssr - m_ur.ssr) / 1) / (m_ur.ssr / (N - 4))\np_val = 1 - stats.f.cdf(f_stat, 1, N - 4)\n\nprint(f\"\\nF-test for equal betas: F = {f_stat:.4f}, p = {p_val:.4f}\")\nprint(\"Reject H0: betas are different\" if p_val < 0.05 else \"Fail to reject H0\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"AMD:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             AMD_Excess   R-squared:                       0.347\nModel:                            OLS   Adj. R-squared:                  0.332\nMethod:                 Least Squares   F-statistic:                     22.86\nDate:                Wed, 18 Feb 2026   Prob (F-statistic):           2.07e-05\nTime:                        00:44:50   Log-Likelihood:                 30.184\nNo. Observations:                  45   AIC:                            -56.37\nDf Residuals:                      43   BIC:                            -52.75\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0047      0.019      0.247      0.806      -0.034       0.043\nMkt-RF         1.8923      0.396      4.781      0.000       1.094       2.691\n==============================================================================\nOmnibus:                        4.701   Durbin-Watson:                   2.316\nProb(Omnibus):                  0.095   Jarque-Bera (JB):                3.783\nSkew:                           0.699   Prob(JB):                        0.151\nKurtosis:                       3.254   Cond. No.                         21.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nBERKSHIRE:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 Excess   R-squared:                       0.512\nModel:                            OLS   Adj. R-squared:                  0.501\nMethod:                 Least Squares   F-statistic:                     45.19\nDate:                Wed, 18 Feb 2026   Prob (F-statistic):           3.26e-08\nTime:                        00:44:50   Log-Likelihood:                 81.742\nNo. Observations:                  45   AIC:                            -159.5\nDf Residuals:                      43   BIC:                            -155.9\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0057      0.006      0.941      0.352      -0.007       0.018\nMkt-RF         0.8460      0.126      6.722      0.000       0.592       1.100\n==============================================================================\nOmnibus:                        1.034   Durbin-Watson:                   1.815\nProb(Omnibus):                  0.596   Jarque-Bera (JB):                1.005\nSkew:                           0.204   Prob(JB):                        0.605\nKurtosis:                       2.392   Cond. No.                         21.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nAMD Beta: 1.8923\nBerkshire Beta: 0.8460\n\nF-test for equal betas: F = 6.3459, p = 0.0136\nReject H0: betas are different\n"}],"execution_count":34},{"id":"a46c2cfc-8fcc-4edd-9af1-1adb11c6a441","cell_type":"code","source":"# QUESTION 3: FAMA-FRENCH 3-FACTOR MODEL\ny = data['AMD_Excess']\nX_ff3 = sm.add_constant(data[['Mkt-RF', 'SMB', 'HML']])\nff3 = sm.OLS(y, X_ff3).fit()\n\nprint(ff3.summary())\n\n# Task 1: R² comparison\nprint(f\"\\nCAPM R²: {r2:.4f} | FF3 R²: {ff3.rsquared:.4f}\")\nprint(f\"Improvement: {(ff3.rsquared - r2)*100:.2f}%\")\n\n# Task 2: F-test for joint significance of SMB and HML\nssr_r = capm.ssr   # restricted model (CAPM)\nssr_u = ff3.ssr    # unrestricted model (FF3)\nn = len(y)\nq = 2\nk = 4\nf_stat = ((ssr_r - ssr_u) / q) / (ssr_u / (n - k))\np_val = 1 - stats.f.cdf(f_stat, q, n - k)\nprint(f\"\\nF-test: F = {f_stat:.4f}, p-value = {p_val:.4f}\")\n\n# Task 3: SMB vs HML sensitivity\nprint(f\"\\nSMB coeff: {ff3.params['SMB']:.4f} (p = {ff3.pvalues['SMB']:.4f})\")\nprint(f\"HML coeff: {ff3.params['HML']:.4f} (p = {ff3.pvalues['HML']:.4f})\")\n\n# Task 4: Alpha comparison\nprint(f\"\\nCAPM Alpha: {alpha:.4f} (p = {alpha_pval:.4f})\")\nprint(f\"FF3 Alpha:  {ff3.params['const']:.4f} (p = {ff3.pvalues['const']:.4f})\")","metadata":{"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             AMD_Excess   R-squared:                       0.409\nModel:                            OLS   Adj. R-squared:                  0.366\nMethod:                 Least Squares   F-statistic:                     9.459\nDate:                Tue, 17 Feb 2026   Prob (F-statistic):           7.13e-05\nTime:                        16:38:40   Log-Likelihood:                 32.427\nNo. Observations:                  45   AIC:                            -56.85\nDf Residuals:                      41   BIC:                            -49.63\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0065      0.019      0.334      0.740      -0.033       0.045\nMkt-RF         1.8126      0.403      4.495      0.000       0.998       2.627\nSMB           -0.3085      0.649     -0.475      0.637      -1.619       1.002\nHML           -0.8511      0.426     -1.996      0.053      -1.712       0.010\n==============================================================================\nOmnibus:                        5.327   Durbin-Watson:                   2.340\nProb(Omnibus):                  0.070   Jarque-Bera (JB):                4.392\nSkew:                           0.751   Prob(JB):                        0.111\nKurtosis:                       3.289   Cond. No.                         36.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nCAPM R²: 0.3471 | FF3 R²: 0.4090\nImprovement: 6.20%\n\nF-test: F = 2.1491, p-value = 0.1295\n\nSMB coeff: -0.3085 (p = 0.6371)\nHML coeff: -0.8511 (p = 0.0526)\n\nCAPM Alpha: 0.0047 (p = 0.8061)\nFF3 Alpha:  0.0065 (p = 0.7398)\n"}],"execution_count":33},{"id":"c4a4a7f2-5cfc-45f7-b43b-3c5731f672a8","cell_type":"code","source":"# QUESTION 4: INDUSTRY PORTFOLIO ANALYSIS\n\n# Load industry data\nindustries = pd.read_excel('5_Industry_Portfolios.xlsx', sheet_name='VW Monthly', skiprows=2)\nind = industries[['Date', 'HiTec', 'Hlth']].copy()\nind.columns = ['Date', 'Tech', 'Health']\nind['Date'] = pd.to_numeric(ind['Date'], errors='coerce')\nind = ind.dropna(subset=['Date'])\nind['Date'] = pd.to_datetime(ind['Date'].astype(int).astype(str), format='%Y%m')\nind = ind.set_index('Date')\n\n# Merge with FF data\nff_q4 = ff.set_index('Date')[['Mkt-RF', 'SMB', 'HML', 'RF']]\nind_data = ind.join(ff_q4, how='inner')\nind_data[['Tech', 'Health']] /= 100\nind_data['Tech_ex'] = ind_data['Tech'] - ind_data['RF']\nind_data['Health_ex'] = ind_data['Health'] - ind_data['RF']\n\nfor col in ['Tech_ex', 'Health_ex', 'Mkt-RF', 'SMB', 'HML']:\n    ind_data[col] = pd.to_numeric(ind_data[col], errors='coerce')\nind_data = ind_data.dropna(subset=['Tech_ex', 'Health_ex', 'Mkt-RF', 'SMB', 'HML'])\n\n# Run FF3 regressions\nX = sm.add_constant(ind_data[['Mkt-RF', 'SMB', 'HML']])\ntech_model = sm.OLS(ind_data['Tech_ex'], X).fit()\nhealth_model = sm.OLS(ind_data['Health_ex'], X).fit()\n\nprint(\"TECHNOLOGY:\")\nprint(tech_model.summary())\nprint(\"\\nHEALTHCARE:\")\nprint(health_model.summary())\n\n# Task 1: Market beta comparison\nprint(f\"\\nTech beta: {tech_model.params['Mkt-RF']:.4f}\")\nprint(f\"Health beta: {health_model.params['Mkt-RF']:.4f}\")\n\n# Task 2: SMB exposure\nprint(f\"\\nTech SMB: {tech_model.params['SMB']:.4f} (p = {tech_model.pvalues['SMB']:.4f})\")\nprint(f\"Health SMB: {health_model.params['SMB']:.4f} (p = {health_model.pvalues['SMB']:.4f})\")\n\n# Task 3: F-test for equal market betas\nt = ind_data[['Tech_ex', 'Mkt-RF', 'SMB', 'HML']].rename(columns={'Tech_ex': 'R'})\nh = ind_data[['Health_ex', 'Mkt-RF', 'SMB', 'HML']].rename(columns={'Health_ex': 'R'})\nt['D'] = 1\nh['D'] = 0\npooled = pd.concat([t, h]).dropna()\npooled['D_Mkt'] = pooled['D'] * pooled['Mkt-RF']\n\nm_r = sm.OLS(pooled['R'], sm.add_constant(pooled[['D', 'Mkt-RF', 'SMB', 'HML']])).fit()\nm_ur = sm.OLS(pooled['R'], sm.add_constant(pooled[['D', 'Mkt-RF', 'SMB', 'HML', 'D_Mkt']])).fit()\n\nf_stat = ((m_r.ssr - m_ur.ssr) / 1) / (m_ur.ssr / (len(pooled) - 6))\np_val = 1 - stats.f.cdf(f_stat, 1, len(pooled) - 6)\nprint(f\"\\nF-test for equal betas: F = {f_stat:.4f}, p = {p_val:.4f}\")\n\n# Task 4: Recession strategy\ntech_drop = tech_model.params['const'] + tech_model.params['Mkt-RF'] * (-0.10)\nhealth_drop = health_model.params['const'] + health_model.params['Mkt-RF'] * (-0.10)\nprint(f\"\\nIf market drops 10%: Tech = {tech_drop:.4f}, Health = {health_drop:.4f}\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"TECHNOLOGY:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                Tech_ex   R-squared:                       0.860\nModel:                            OLS   Adj. R-squared:                  0.859\nMethod:                 Least Squares   F-statistic:                     2409.\nDate:                Tue, 17 Feb 2026   Prob (F-statistic):               0.00\nTime:                        16:38:44   Log-Likelihood:                 2896.1\nNo. Observations:                1183   AIC:                            -5784.\nDf Residuals:                    1179   BIC:                            -5764.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0011      0.001     -1.862      0.063      -0.002    6.15e-05\nMkt-RF         0.9918      0.012     80.571      0.000       0.968       1.016\nSMB            0.0364      0.020      1.798      0.072      -0.003       0.076\nHML           -0.3036      0.018    -17.260      0.000      -0.338      -0.269\n==============================================================================\nOmnibus:                       91.677   Durbin-Watson:                   1.977\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              386.474\nSkew:                           0.226   Prob(JB):                     1.20e-84\nKurtosis:                       5.763   Cond. No.                         34.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nHEALTHCARE:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              Health_ex   R-squared:                       0.661\nModel:                            OLS   Adj. R-squared:                  0.660\nMethod:                 Least Squares   F-statistic:                     766.6\nDate:                Tue, 17 Feb 2026   Prob (F-statistic):          2.16e-276\nTime:                        16:38:44   Log-Likelihood:                 2393.4\nNo. Observations:                1183   AIC:                            -4779.\nDf Residuals:                    1179   BIC:                            -4758.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0001      0.001      0.112      0.911      -0.002       0.002\nMkt-RF         0.8731      0.019     46.371      0.000       0.836       0.910\nSMB           -0.0879      0.031     -2.836      0.005      -0.149      -0.027\nHML           -0.1774      0.027     -6.594      0.000      -0.230      -0.125\n==============================================================================\nOmnibus:                       84.414   Durbin-Watson:                   1.999\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              370.437\nSkew:                          -0.138   Prob(JB):                     3.64e-81\nKurtosis:                       5.727   Cond. No.                         34.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nTech beta: 0.9918\nHealth beta: 0.8731\n\nTech SMB: 0.0364 (p = 0.0725)\nHealth SMB: -0.0879 (p = 0.0047)\n\nF-test for equal betas: F = 34.2443, p = 0.0000\n\nIf market drops 10%: Tech = -0.1003, Health = -0.0872\n"}],"execution_count":34},{"id":"264d3830-86c2-445b-827d-c9e8d22b2184","cell_type":"code","source":"# QUESTION 5: FAMA-FRENCH FIVE-FACTOR MODEL\nimport statsmodels.formula.api as smf\n# load stock returns (same as Q1)\namd = pd.read_excel('AMD - Stock return.xlsx').dropna(subset=['Stock Return'])\namd['Date'] = pd.to_datetime(amd['Date'])\n\n# load 5-factor data (header is row 5 in Excel = skiprows=4)\nff5 = pd.read_excel('F-F_Research_Data_5_Factors_2x3.xlsx', skiprows=4)\nff5.columns = [c.strip() for c in ff5.columns]\nff5.rename(columns={ff5.columns[0]: 'Date'}, inplace=True)\nff5['Date'] = pd.to_numeric(ff5['Date'], errors='coerce')\nff5 = ff5.dropna(subset=['Date'])\nff5['Date'] = ff5['Date'].astype(int).astype(str)\nff5 = ff5[ff5['Date'].str.len() == 6]\nff5['Date'] = pd.to_datetime(ff5['Date'], format='%Y%m')\nfor c in ['Mkt-RF','SMB','HML','RMW','CMA','RF']:\n    ff5[c] = pd.to_numeric(ff5[c], errors='coerce') / 100\n\n\n# merge\ndf5 = pd.merge(amd, ff5, on='Date', how='inner').dropna()\ndf5['AMD_Excess'] = df5['Stock Return'] - df5['RF']\nprint(f\"Observations: {len(df5)}\")\n\n# run all three models\ncapm = smf.ols(\"AMD_Excess ~ Q('Mkt-RF')\", data=df5).fit()\nmodel_3F = smf.ols(\"AMD_Excess ~ Q('Mkt-RF') + SMB + HML\", data=df5).fit()\nmodel_5F = smf.ols(\"AMD_Excess ~ Q('Mkt-RF') + SMB + HML + RMW + CMA\", data=df5).fit()\n\nprint(capm.summary())\nprint(model_3F.summary())\nprint(model_5F.summary())\n\n# task 1: test if all five factors are jointly significant\n# H0: all betas = 0\nrestricted = smf.ols(\"AMD_Excess ~ 1\", data=df5).fit()\nF_all = model_5F.compare_f_test(restricted)\n\nr2 = model_5F.rsquared\nk, n = 5, len(df5)\nF_manual = (r2 / k) / ((1 - r2) / (n - k - 1))\n\nprint(f\"\\nTest 1 - joint significance of all factors:\")\nprint(f\"  F-stat (Python):  {F_all[0]:.4f},  p = {F_all[1]:.4f}\")\nprint(f\"  F-stat (manual):  {F_manual:.4f}\")\n\n# task 2: do RMW and CMA add anything beyond the 3-factor model?\n# H0: beta_RMW = beta_CMA = 0\nF_incr = model_5F.compare_f_test(model_3F)\nprint(f\"\\nTest 2 - RMW & CMA incremental test:\")\nprint(f\"  F-stat: {F_incr[0]:.4f},  p = {F_incr[1]:.4f}\")\n\n# task 3: test if SMB and HML coefficients are equal\n# H0: beta_SMB = beta_HML\nF_equal = model_5F.f_test(\"SMB = HML\")\nprint(f\"\\nTest 3 - SMB = HML:\")\nprint(F_equal)\n\n# task 4: model comparison\nprint(f\"\\nModel Comparison:\")\nprint(f\"  CAPM     - Adj R2: {capm.rsquared_adj:.5f},  AIC: {capm.aic:.2f}\")\nprint(f\"  3-Factor - Adj R2: {model_3F.rsquared_adj:.5f},  AIC: {model_3F.aic:.2f}\")\nprint(f\"  5-Factor - Adj R2: {model_5F.rsquared_adj:.5f},  AIC: {model_5F.aic:.2f}\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Observations: 57\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             AMD_Excess   R-squared:                       0.277\nModel:                            OLS   Adj. R-squared:                  0.264\nMethod:                 Least Squares   F-statistic:                     21.11\nDate:                Wed, 18 Feb 2026   Prob (F-statistic):           2.58e-05\nTime:                        00:45:41   Log-Likelihood:                 30.734\nNo. Observations:                  57   AIC:                            -57.47\nDf Residuals:                      55   BIC:                            -53.38\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept       0.0120      0.019      0.620      0.537      -0.027       0.051\nQ('Mkt-RF')     1.9499      0.424      4.594      0.000       1.099       2.800\n==============================================================================\nOmnibus:                       18.476   Durbin-Watson:                   2.391\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               24.680\nSkew:                           1.218   Prob(JB):                     4.37e-06\nKurtosis:                       5.112   Cond. No.                         22.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             AMD_Excess   R-squared:                       0.359\nModel:                            OLS   Adj. R-squared:                  0.323\nMethod:                 Least Squares   F-statistic:                     9.897\nDate:                Wed, 18 Feb 2026   Prob (F-statistic):           2.77e-05\nTime:                        00:45:41   Log-Likelihood:                 34.154\nNo. Observations:                  57   AIC:                            -60.31\nDf Residuals:                      53   BIC:                            -52.14\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept       0.0159      0.020      0.815      0.419      -0.023       0.055\nQ('Mkt-RF')     1.7850      0.432      4.135      0.000       0.919       2.651\nSMB            -0.2818      0.676     -0.417      0.679      -1.639       1.075\nHML            -1.0946      0.480     -2.279      0.027      -2.058      -0.131\n==============================================================================\nOmnibus:                       18.331   Durbin-Watson:                   2.405\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               23.856\nSkew:                           1.234   Prob(JB):                     6.60e-06\nKurtosis:                       4.988   Cond. No.                         39.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             AMD_Excess   R-squared:                       0.364\nModel:                            OLS   Adj. R-squared:                  0.302\nMethod:                 Least Squares   F-statistic:                     5.840\nDate:                Wed, 18 Feb 2026   Prob (F-statistic):           0.000244\nTime:                        00:45:41   Log-Likelihood:                 34.377\nNo. Observations:                  57   AIC:                            -56.75\nDf Residuals:                      51   BIC:                            -44.50\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept       0.0151      0.020      0.752      0.455      -0.025       0.056\nQ('Mkt-RF')     1.7757      0.440      4.039      0.000       0.893       2.658\nSMB            -0.4894      0.766     -0.639      0.526      -2.027       1.048\nHML            -0.8003      0.738     -1.084      0.284      -2.283       0.682\nRMW            -0.3237      0.801     -0.404      0.688      -1.932       1.284\nCMA            -0.4197      0.978     -0.429      0.670      -2.384       1.544\n==============================================================================\nOmnibus:                       16.670   Durbin-Watson:                   2.359\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               19.990\nSkew:                           1.198   Prob(JB):                     4.56e-05\nKurtosis:                       4.637   Cond. No.                         63.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nTest 1 - joint significance of all factors:\n  F-stat (Python):  5.8396,  p = 0.0002\n  F-stat (manual):  5.8396\n\nTest 2 - RMW & CMA incremental test:\n  F-stat: 0.2008,  p = 0.8187\n\nTest 3 - SMB = HML:\n<F test: F=0.0601378425370217, p=0.8072623276569773, df_denom=51, df_num=1>\n\nModel Comparison:\n  CAPM     - Adj R2: 0.26421,  AIC: -57.47\n  3-Factor - Adj R2: 0.32279,  AIC: -60.31\n  5-Factor - Adj R2: 0.30173,  AIC: -56.75\n"}],"execution_count":35},{"id":"e02d42ea-92b9-41ac-af0a-92e562ad97a3","cell_type":"code","source":"# QUESTION 6: LONG-TERM REVERSAL EFFECT\n\n# Load reversal portfolios\nrev = pd.read_csv(\"10_Portfolios_Prior_60_13.CSV\", skiprows=10, header=0)\nrev.rename(columns={'Unnamed: 0': 'YM'}, inplace=True)\nrev['YM'] = rev['YM'].astype(str).str.strip()\nrev = rev[rev['YM'].str.match(r'^\\d{6}$')]\nrev = rev.apply(lambda c: pd.to_numeric(c, errors='coerce')).replace([-99.99, -999], np.nan)\n# Keep only first section (value-weighted) — file has VW then EW stacked\nrev = rev.iloc[:len(rev)//2]\nrev.columns = ['YM','P1','P2','P3','P4','P5','P6','P7','P8','P9','P10']\nrev['YM'] = rev['YM'].astype(int).astype(str)\n\n\n# Load FF factors\nff_q6 = pd.read_csv(\"F-F_Research_Data_Factors.CSV\", skiprows=3, header=0, index_col=0)\nff_q6.index = ff_q6.index.astype(str).str.strip()\nff_q6 = ff_q6[ff_q6.index.str.match(r'^\\d{6}$')]\nff_q6 = ff_q6.apply(lambda c: pd.to_numeric(c, errors='coerce'))\nff_q6.columns = [c.strip() for c in ff_q6.columns]\nff_q6['YM'] = ff_q6.index.str.strip()\n\n# Merge on year-month\ndf_q6 = pd.merge(rev, ff_q6[['YM','Mkt-RF','RF']], on='YM', how='inner').dropna().reset_index(drop=True)\n\n\nloser = df_q6['P1'].values\nwinner = df_q6['P10'].values\nreversal = loser - winner\n\n# Task 1: average monthly returns\nprint(f\"\\nLoser (P1) avg return: {loser.mean():.4f}%\")\nprint(f\"Winner (P10) avg return: {winner.mean():.4f}%\")\n\n# Task 2: reversal strategy return\nprint(f\"Long-Short avg return: {reversal.mean():.4f}% per month\")\n\n# Task 3: t-test on reversal premium\nn = len(reversal)\nmean_r = reversal.mean()\nse_r = reversal.std(ddof=1) / np.sqrt(n)\nt_stat = mean_r / se_r\np_val = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-1))\nprint(f\"\\nt-test: t={t_stat:.4f}, p={p_val:.4f}\")\nprint(f\"Mean: {mean_r:.4f}%, SE: {se_r:.4f}%\")\n\n# Task 4: compare 20th vs 21st century\ndef ttest(series):\n    m = series.mean()\n    se = series.std(ddof=1) / np.sqrt(len(series))\n    t = m / se\n    p = 2 * (1 - stats.t.cdf(abs(t), df=len(series)-1))\n    return m, t, p\n\nyears = df_q6['YM'].str[:4].astype(int).values\nm20, t20, p20 = ttest(reversal[years < 2000])\nm21, t21, p21 = ttest(reversal[years >= 2000])\nprint(f\"\\n20th century: mean={m20:.4f}%, t={t20:.4f}, p={p20:.4f}\")\nprint(f\"21st century: mean={m21:.4f}%, t={t21:.4f}, p={p21:.4f}\")\n\n# Task 5: CAPM regression on the reversal portfolio\ny = reversal - df_q6['RF'].values\nx = df_q6['Mkt-RF'].values\n\ngood = np.isfinite(y) & np.isfinite(x)\ny, x = y[good], x[good]\n\nX = np.column_stack([np.ones(len(x)), x])\nb = np.linalg.lstsq(X, y, rcond=None)[0]\nalpha_rev, beta_rev = b\n\nresid = y - X @ b\nmse = np.sum(resid**2) / (len(y) - 2)\ncov_mat = mse * np.linalg.pinv(X.T @ X)\n\nt_a = alpha_rev / np.sqrt(cov_mat[0,0])\nt_b = beta_rev / np.sqrt(cov_mat[1,1])\np_a = 2 * (1 - stats.t.cdf(abs(t_a), df=len(y)-2))\np_b = 2 * (1 - stats.t.cdf(abs(t_b), df=len(y)-2))\nr2_rev = 1 - np.sum(resid**2) / np.sum((y - y.mean())**2)\n\nprint(f\"\\nCAPM: Alpha={alpha_rev:.4f}% (t={t_a:.4f}, p={p_a:.4f})\")\nprint(f\"      Beta={beta_rev:.4f} (t={t_b:.4f}, p={p_b:.4f})\")\nprint(f\"      R2={r2_rev:.4f}\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"\nLoser (P1) avg return: 1.6886%\nWinner (P10) avg return: 0.9468%\nLong-Short avg return: 0.7418% per month\n\nt-test: t=4.9606, p=0.0000\nMean: 0.7418%, SE: 0.1495%\n\n20th century: mean=0.9623%, t=5.1923, p=0.0000\n21st century: mean=0.1333%, t=0.5749, p=0.5656\n\nCAPM: Alpha=0.2628% (t=1.7820, p=0.0749)\n      Beta=0.2984 (t=10.7737, p=0.0000)\n      R2=0.0490\n"}],"execution_count":36},{"id":"a713f64c-75e1-4ead-8fe6-564beb3027e0","cell_type":"code","source":"# QUESTION 7: IS THE VALUE PREMIUM DEAD?\n\n# load book-to-market sorted portfolios\nport = pd.read_excel(\"100_Portfolios_10x10.xlsx\", header=15, index_col=0)\nport.index = port.index.astype(str).str.strip()\nport = port[port.index.str.fullmatch(r\"\\d{6}\")]\n\n# drop unnamed columns and clean\nport = port[[c for c in port.columns if 'Unnamed' not in str(c)]]\nport = port.apply(pd.to_numeric, errors='coerce').replace([-99.99, -999], np.nan)\nport = port.dropna(how='all') / 100\nport.index = pd.to_datetime(port.index, format='%Y%m')\n\n\n# Growth = first col (low B/M), Value = last col (high B/M)\ngrowth = port.iloc[:, 0]\nvalue = port.iloc[:, -1]\n\n# task 1: average monthly returns\nprint(f\"\\nTask 1:\")\nprint(f\"  Growth avg return: {growth.mean():.5f}\")\nprint(f\"  Value avg return:  {value.mean():.5f}\")\nprint(f\"  Growth = low book-to-market (growth stocks)\")\nprint(f\"  Value = high book-to-market (value stocks)\")\n\n# task 2: long-short portfolio (long value, short growth)\nvalue_premium = value - growth\n\nprint(f\"\\nTask 2:\")\nprint(f\"  Value premium (long value, short growth): {value_premium.mean():.5f} per month\")\n\n# task 3: t-tests by century\nyears = port.index.year\n\nvp_20c = value_premium[(years >= 1963) & (years <= 1999)].dropna()\nvp_21c = value_premium[years >= 2000].dropna()\n\nt20 = stats.ttest_1samp(vp_20c, 0)\nt21 = stats.ttest_1samp(vp_21c, 0)\n\nprint(f\"\\nTask 3:\")\nprint(f\"  20th century: mean={vp_20c.mean():.5f}, t={t20.statistic:.3f}, p={t20.pvalue:.5f}\")\nprint(f\"  21st century: mean={vp_21c.mean():.5f}, t={t21.statistic:.3f}, p={t21.pvalue:.5f}\")\n\n# task 4: is the value premium different between centuries?\ndiff_test = stats.ttest_ind(vp_20c, vp_21c, equal_var=False)\n\n# manual calculation\nm1, m2 = vp_20c.mean(), vp_21c.mean()\nv1, v2 = vp_20c.var(ddof=1), vp_21c.var(ddof=1)\nn1, n2 = len(vp_20c), len(vp_21c)\nt_manual = (m1 - m2) / np.sqrt(v1/n1 + v2/n2)\n\nprint(f\"\\nTask 4:\")\nprint(f\"  Difference in means t-stat (Python): {diff_test.statistic:.3f}, p={diff_test.pvalue:.5f}\")\nprint(f\"  Difference in means t-stat (manual): {t_manual:.3f}\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"\nTask 1:\n  Growth avg return: 0.00949\n  Value avg return:  0.01096\n  Growth = low book-to-market (growth stocks)\n  Value = high book-to-market (value stocks)\n\nTask 2:\n  Value premium (long value, short growth): 0.00007 per month\n\nTask 3:\n  20th century: mean=0.00132, t=0.407, p=0.68404\n  21st century: mean=-0.00197, t=-0.470, p=0.63888\n\nTask 4:\n  Difference in means t-stat (Python): 0.621, p=0.53497\n  Difference in means t-stat (manual): 0.621\n"}],"execution_count":37},{"id":"9bcfe1d8-0755-429b-9cec-d1c5c6b03053","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}